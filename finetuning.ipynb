{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mistral-finetune'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mistralai/mistral-finetune.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/mistral-finetune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd mistral-finetune/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.27.1\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-08 11:06:43--  https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar\n",
      "Resolving models.mistralcdn.com (models.mistralcdn.com)... 104.26.7.117, 172.67.70.68, 104.26.6.117, ...\n",
      "Connecting to models.mistralcdn.com (models.mistralcdn.com)|104.26.7.117|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14496675840 (14G) [application/x-tar]\n",
      "Saving to: â€˜mistral-7B-v0.3.tar.1â€™\n",
      "\n",
      "mistral-7B-v0.3.tar   0%[                    ] 132.13M  27.9MB/s    eta 8m 55s ^C\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, you can download the model from mistral\n",
    "\n",
    "!wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!DIR=~/mistral_models && mkdir -p $DIR && tar -xf mistral-7B-v0.3.tar -C $DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the training data doesn't have the right format,\n",
    "# so we need to reformat the data into the correct format and skip the cases that don't have the right format:\n",
    "\n",
    "!python -m utils.reformat_data /teamspace/studios/this_studio/mistral-finetune/data/mistral.train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Validating /teamspace/studios/this_studio/mistral-finetune/data/mistral_train_dataset.jsonl ...\n",
      "\n",
      "  0%|                                                 | 0/29999 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|â–‰                                    | 760/29999 [00:00<00:03, 7595.88it/s]\u001b[A\n",
      "  5%|â–ˆâ–‰                                  | 1622/29999 [00:00<00:03, 8194.56it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–‰                                 | 2493/29999 [00:00<00:03, 8428.81it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆ                                | 3372/29999 [00:00<00:03, 8567.90it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 4229/29999 [00:00<00:03, 8567.70it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 5090/29999 [00:00<00:02, 8578.97it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 5948/29999 [00:00<00:02, 8353.97it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 6785/29999 [00:00<00:02, 8100.85it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 7655/29999 [00:00<00:02, 8281.44it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 8511/29999 [00:01<00:02, 8364.01it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 9349/29999 [00:01<00:02, 8353.15it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 10188/29999 [00:01<00:02, 8364.07it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 11026/29999 [00:01<00:02, 7963.89it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 11827/29999 [00:01<00:02, 7706.07it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 12602/29999 [00:01<00:02, 7541.93it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 13359/29999 [00:01<00:02, 7423.97it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 14104/29999 [00:01<00:02, 7319.50it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 14838/29999 [00:01<00:02, 7296.01it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 15569/29999 [00:01<00:01, 7298.88it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 16300/29999 [00:02<00:01, 7013.62it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 17004/29999 [00:02<00:01, 6901.04it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 17707/29999 [00:02<00:01, 6936.01it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 18421/29999 [00:02<00:01, 6994.27it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 19128/29999 [00:02<00:01, 7015.27it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 19839/29999 [00:02<00:01, 7040.69it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 20738/29999 [00:02<00:01, 7618.15it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 21661/29999 [00:02<00:01, 8097.11it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 22596/29999 [00:02<00:00, 8470.44it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 23549/29999 [00:02<00:00, 8785.64it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 24464/29999 [00:03<00:00, 8894.45it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25424/29999 [00:03<00:00, 9104.42it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 26374/29999 [00:03<00:00, 9220.72it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27320/29999 [00:03<00:00, 9292.16it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 28265/29999 [00:03<00:00, 9338.20it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29999/29999 [00:03<00:00, 8164.35it/s]\u001b[A\n",
      "1it [00:03,  3.73s/it]\n",
      "No errors! Data is correctly formatted!\n",
      "Stats for /teamspace/studios/this_studio/mistral-finetune/data/mistral_train_dataset.jsonl \n",
      " -------------------- \n",
      " {\n",
      "    \"expected\": {\n",
      "        \"eta\": \"00:22:54\",\n",
      "        \"data_tokens\": 2046750,\n",
      "        \"train_tokens\": 49152000,\n",
      "        \"epochs\": \"24.01\",\n",
      "        \"max_steps\": 750,\n",
      "        \"data_tokens_per_dataset\": {\n",
      "            \"/teamspace/studios/this_studio/mistral-finetune/data/mistral_train_dataset.jsonl\": \"2046750.0\"\n",
      "        },\n",
      "        \"train_tokens_per_dataset\": {\n",
      "            \"/teamspace/studios/this_studio/mistral-finetune/data/mistral_train_dataset.jsonl\": \"49152000.0\"\n",
      "        },\n",
      "        \"epochs_per_dataset\": {\n",
      "            \"/teamspace/studios/this_studio/mistral-finetune/data/mistral_train_dataset.jsonl\": \"24.0\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "0it [00:00, ?it/s]Validating /teamspace/studios/this_studio/mistral-finetune/data/mistral_val_dataset.jsonl ...\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:00<00:00, 7815.03it/s]\u001b[A\n",
      "1it [00:00, 14.59it/s]\n",
      "No errors! Data is correctly formatted!\n"
     ]
    }
   ],
   "source": [
    "# Now you can verify your training yaml to make sure the data is correctly formatted and to get an estimate of your training time.\n",
    "\n",
    "!python -m utils.validate_data --train_yaml example/7B.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training configuration\n",
    "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
    "\n",
    "config = \"\"\"\n",
    "# data\n",
    "data:\n",
    "  instruct_data: \"/teamspace/studios/this_studio/mistral-finetune/data/mistral_train_dataset.jsonl\"\n",
    "  data: \"\" \n",
    "  eval_instruct_data: \"/teamspace/studios/this_studio/mistral-finetune/data/mistral_val_dataset.jsonl\"\n",
    "\n",
    "# model\n",
    "model_id_or_path: \"/teamspace/studios/this_studio/mistral_models\"\n",
    "lora:\n",
    "  rank: 64  # Keep low-rank adapters to fit the model on available memory\n",
    "\n",
    "# optim\n",
    "seq_len: 2048  # Max sequence length; adjust if you face memory constraints\n",
    "batch_size: 2  # Increase to 4 if memory permits for faster training\n",
    "max_steps: 500  # Slightly increase for sufficient epochs over 30k dataset\n",
    "optim:\n",
    "  lr: 4e-5  # Lower learning rate for better stability with fine-tuning\n",
    "  weight_decay: 0.05  # Slightly lower to avoid over-regularization\n",
    "  pct_start: 0.1  # Adjust warm-up period for smoother LR schedule\n",
    "\n",
    "# other\n",
    "seed: 42  # Reproducibility\n",
    "log_freq: 10  # Log every 10 steps\n",
    "eval_freq: 50  # Evaluate more frequently for smaller datasets\n",
    "no_eval: False\n",
    "ckpt_freq: 100  # Save checkpoints frequently to avoid data loss\n",
    "\n",
    "save_adapters: True  # Save LoRA adapters only; save full model if needed by setting to False\n",
    "\n",
    "run_dir: \"/teamspace/studios/this_studio/output_lora\"\n",
    "\n",
    "wandb:\n",
    "  project: \"Mistral-7B-v3\"\n",
    "  run_name: \"fine-tuning-mistral\"\n",
    "  key: \"057a18c9b0ad17d5aa800a60fc6907dfb87c226e\"\n",
    "  offline: False\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# save the same file locally into the example.yaml file\n",
    "import yaml\n",
    "with open('example.yaml', 'w') as file:\n",
    "    yaml.dump(yaml.safe_load(config), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/mistral-finetune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# these info is needed for training\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%cd mistral-finetune/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:You have disabled `save_adapters` and are thus merging the trained LoRA checkpoint into the base model upon checkpointing. This might lead to OOM errors - make sure you have enough CPU and GPU memory.\n",
      "args: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/teamspace/studios/this_studio/mistral-finetune/data/mistral.train.jsonl', eval_instruct_data='/teamspace/studios/this_studio/mistral-finetune/data/mistral.val.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/teamspace/studios/this_studio/mistral_models', run_dir='/teamspace/studios/this_studio/output_mixed_data_v2', optim=OptimArgs(lr=2e-05, weight_decay=0.05, pct_start=0.1), seed=0, num_microbatches=1, seq_len=2048, batch_size=4, max_norm=1.0, max_steps=300, log_freq=10, ckpt_freq=60, save_adapters=False, no_ckpt=False, num_ckpt_keep=3, eval_freq=50, no_eval=False, checkpoint=True, world_size=1, wandb=WandbArgs(project='Mistral-7B-v3', offline=False, key='057a18c9b0ad17d5aa800a60fc6907dfb87c226e', run_name='fine-tuning-mistral'), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=32, dropout=0.0, scaling=2.0))\n",
      "2025-01-09 07:05:50 (UTC) - 0:00:04 - distributed - INFO - torch.cuda.device_count: 1\n",
      "2025-01-09 07:05:50 (UTC) - 0:00:04 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0\n",
      "2025-01-09 07:05:50 (UTC) - 0:00:04 - distributed - INFO - local rank: 0\n",
      "2025-01-09 07:05:50 (UTC) - 0:00:04 - train - INFO - Going to init comms...\n",
      "2025-01-09 07:05:50 (UTC) - 0:00:04 - train - INFO - Run dir: /teamspace/studios/this_studio/output_mixed_data_v2\n",
      "2025-01-09 07:05:51 (UTC) - 0:00:05 - train - INFO - TrainArgs: {'batch_size': 4,\n",
      " 'checkpoint': True,\n",
      " 'ckpt_freq': 60,\n",
      " 'data': {'data': '',\n",
      "          'eval_instruct_data': '/teamspace/studios/this_studio/mistral-finetune/data/mistral.val.jsonl',\n",
      "          'instruct': {'dynamic_chunk_fn_call': True, 'shuffle': True},\n",
      "          'instruct_data': '/teamspace/studios/this_studio/mistral-finetune/data/mistral.train.jsonl',\n",
      "          'shuffle': False},\n",
      " 'eval_freq': 50,\n",
      " 'log_freq': 10,\n",
      " 'lora': {'dropout': 0.0, 'enable': True, 'rank': 32, 'scaling': 2.0},\n",
      " 'max_norm': 1.0,\n",
      " 'max_steps': 300,\n",
      " 'mlflow': {'experiment_name': None, 'tracking_uri': None},\n",
      " 'model_id_or_path': '/teamspace/studios/this_studio/mistral_models',\n",
      " 'no_ckpt': False,\n",
      " 'no_eval': False,\n",
      " 'num_ckpt_keep': 3,\n",
      " 'num_microbatches': 1,\n",
      " 'optim': {'lr': 2e-05, 'pct_start': 0.1, 'weight_decay': 0.05},\n",
      " 'run_dir': '/teamspace/studios/this_studio/output_mixed_data_v2',\n",
      " 'save_adapters': False,\n",
      " 'seed': 0,\n",
      " 'seq_len': 2048,\n",
      " 'wandb': {'key': '057a18c9b0ad17d5aa800a60fc6907dfb87c226e',\n",
      "           'offline': False,\n",
      "           'project': 'Mistral-7B-v3',\n",
      "           'run_name': 'fine-tuning-mistral'},\n",
      " 'world_size': 1}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibaraa\u001b[0m (\u001b[33mibaraahekal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "2025-01-09 07:05:51 (UTC) - 0:00:05 - metrics_logger - INFO - initializing wandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/teamspace/studios/this_studio/output_mixed_data_v2/wandb/run-20250109_070551-4tt2b50b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-tuning-mistral\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ibaraahekal/Mistral-7B-v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ibaraahekal/Mistral-7B-v3/runs/4tt2b50b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "2025-01-09 07:05:52 (UTC) - 0:00:06 - finetune.wrapped_model - INFO - Reloading model from /teamspace/studios/this_studio/mistral_models/consolidated.safetensors ...\n",
      "2025-01-09 07:06:22 (UTC) - 0:00:36 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...\n",
      "2025-01-09 07:06:22 (UTC) - 0:00:36 - finetune.wrapped_model - INFO - Loaded model on cpu!\n",
      "2025-01-09 07:06:22 (UTC) - 0:00:36 - finetune.wrapped_model - INFO - Initializing lora layers ...\n",
      "2025-01-09 07:06:23 (UTC) - 0:00:37 - finetune.wrapped_model - INFO - Finished initialization!\n",
      "2025-01-09 07:06:23 (UTC) - 0:00:37 - finetune.wrapped_model - INFO - Sharding model over 1 GPUs ...\n",
      "2025-01-09 07:10:43 (UTC) - 0:04:57 - finetune.wrapped_model - INFO - Model sharded!\n",
      "2025-01-09 07:10:43 (UTC) - 0:04:57 - finetune.wrapped_model - INFO - 83,886,080 out of 7,331,909,632 parameters are finetuned (1.14%).\n",
      "2025-01-09 07:10:43 (UTC) - 0:04:57 - dataset - INFO - Loading /teamspace/studios/this_studio/mistral-finetune/data/mistral.train.jsonl ...\n",
      "2025-01-09 07:10:54 (UTC) - 0:05:08 - dataset - INFO - /teamspace/studios/this_studio/mistral-finetune/data/mistral.train.jsonl loaded and tokenized.\n",
      "2025-01-09 07:10:54 (UTC) - 0:05:08 - dataset - INFO - Shuffling /teamspace/studios/this_studio/mistral-finetune/data/mistral.train.jsonl ...\n",
      "2025-01-09 07:11:25 (UTC) - 0:05:39 - train - INFO - step: 000010 - done (%): 3.3 - loss: 1.524 - lr: 5.0e-06 - peak_alloc_mem (GB): 20.8 - alloc_mem (GB): 15.8 - words_per_second: 2711.2 - avg_words_per_second: 1956.9 - ETA: >2025-01-09 07:31:39\n",
      "2025-01-09 07:11:56 (UTC) - 0:06:10 - train - INFO - step: 000020 - done (%): 6.7 - loss: 0.266 - lr: 1.5e-05 - peak_alloc_mem (GB): 20.8 - alloc_mem (GB): 15.8 - words_per_second: 2669.7 - avg_words_per_second: 2261.1 - ETA: >2025-01-09 07:28:50\n",
      "2025-01-09 07:12:27 (UTC) - 0:06:40 - train - INFO - step: 000030 - done (%): 10.0 - loss: 0.188 - lr: 2.0e-05 - peak_alloc_mem (GB): 20.8 - alloc_mem (GB): 15.8 - words_per_second: 2652.4 - avg_words_per_second: 2382.0 - ETA: >2025-01-09 07:27:55\n",
      "2025-01-09 07:12:58 (UTC) - 0:07:12 - train - INFO - step: 000040 - done (%): 13.3 - loss: 0.101 - lr: 2.0e-05 - peak_alloc_mem (GB): 20.8 - alloc_mem (GB): 15.8 - words_per_second: 2625.8 - avg_words_per_second: 2433.5 - ETA: >2025-01-09 07:27:33\n",
      "2025-01-09 07:13:29 (UTC) - 0:07:43 - eval - INFO - Start eval...\n",
      "2025-01-09 07:13:38 (UTC) - 0:07:52 - eval - INFO - Eval finished!\n",
      "2025-01-09 07:13:38 (UTC) - 0:07:52 - train - INFO - step: 000050 - eval_perplexity: 1.134 - eval_loss: 0.182 - train_loss: 0.100\n",
      "2025-01-09 07:13:38 (UTC) - 0:07:52 - train - INFO - step: 000050 - done (%): 16.7 - loss: 0.100 - lr: 2.0e-05 - peak_alloc_mem (GB): 20.8 - alloc_mem (GB): 15.8 - words_per_second: 691.6 - avg_words_per_second: 2350.0 - ETA: >2025-01-09 07:28:09\n",
      "2025-01-09 07:14:09 (UTC) - 0:08:23 - train - INFO - step: 000060 - done (%): 20.0 - loss: 0.059 - lr: 1.9e-05 - peak_alloc_mem (GB): 20.8 - alloc_mem (GB): 15.8 - words_per_second: 2645.9 - avg_words_per_second: 2393.8 - ETA: >2025-01-09 07:27:50\n",
      "2025-01-09 07:14:09 (UTC) - 0:08:23 - checkpointing - INFO - Dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000060/consolidated using tmp name: tmp.consolidated\n",
      "2025-01-09 07:14:39 (UTC) - 0:08:52 - checkpointing - INFO - Done dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000060/consolidated for step: 60\n",
      "2025-01-09 07:14:39 (UTC) - 0:08:52 - checkpointing - INFO - Done deleting checkpoints \n",
      "2025-01-09 07:14:39 (UTC) - 0:08:52 - checkpointing - INFO - Done!\n",
      "2025-01-09 07:15:09 (UTC) - 0:09:23 - train - INFO - step: 000070 - done (%): 23.3 - loss: 0.055 - lr: 1.9e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2652.9 - avg_words_per_second: 2428.0 - ETA: >2025-01-09 07:28:05\n",
      "2025-01-09 07:15:40 (UTC) - 0:09:54 - train - INFO - step: 000080 - done (%): 26.7 - loss: 0.046 - lr: 1.8e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2618.5 - avg_words_per_second: 2452.5 - ETA: >2025-01-09 07:27:55\n",
      "2025-01-09 07:16:12 (UTC) - 0:10:26 - train - INFO - step: 000090 - done (%): 30.0 - loss: 0.058 - lr: 1.8e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2617.5 - avg_words_per_second: 2470.1 - ETA: >2025-01-09 07:27:48\n",
      "2025-01-09 07:16:43 (UTC) - 0:10:57 - eval - INFO - Start eval...\n",
      "2025-01-09 07:16:52 (UTC) - 0:11:05 - eval - INFO - Eval finished!\n",
      "2025-01-09 07:16:52 (UTC) - 0:11:05 - train - INFO - step: 000100 - eval_perplexity: 1.175 - eval_loss: 0.233 - train_loss: 0.068\n",
      "2025-01-09 07:16:52 (UTC) - 0:11:05 - train - INFO - step: 000100 - done (%): 33.3 - loss: 0.068 - lr: 1.7e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 690.8 - avg_words_per_second: 2421.1 - ETA: >2025-01-09 07:28:08\n",
      "2025-01-09 07:17:23 (UTC) - 0:11:36 - train - INFO - step: 000110 - done (%): 36.7 - loss: 0.056 - lr: 1.6e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2651.6 - avg_words_per_second: 2439.5 - ETA: >2025-01-09 07:28:01\n",
      "2025-01-09 07:17:54 (UTC) - 0:12:08 - train - INFO - step: 000120 - done (%): 40.0 - loss: 0.046 - lr: 1.5e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2621.4 - avg_words_per_second: 2453.0 - ETA: >2025-01-09 07:27:55\n",
      "2025-01-09 07:17:54 (UTC) - 0:12:08 - checkpointing - INFO - Dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000120/consolidated using tmp name: tmp.consolidated\n",
      "2025-01-09 07:18:23 (UTC) - 0:12:36 - checkpointing - INFO - Done dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000120/consolidated for step: 120\n",
      "2025-01-09 07:18:23 (UTC) - 0:12:36 - checkpointing - INFO - Done deleting checkpoints \n",
      "2025-01-09 07:18:23 (UTC) - 0:12:36 - checkpointing - INFO - Done!\n",
      "2025-01-09 07:18:54 (UTC) - 0:13:08 - train - INFO - step: 000130 - done (%): 43.3 - loss: 0.050 - lr: 1.4e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2666.3 - avg_words_per_second: 2466.4 - ETA: >2025-01-09 07:28:18\n",
      "2025-01-09 07:19:25 (UTC) - 0:13:39 - train - INFO - step: 000140 - done (%): 46.7 - loss: 0.034 - lr: 1.3e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2643.2 - avg_words_per_second: 2478.1 - ETA: >2025-01-09 07:28:14\n",
      "2025-01-09 07:19:56 (UTC) - 0:14:10 - eval - INFO - Start eval...\n",
      "2025-01-09 07:20:05 (UTC) - 0:14:19 - eval - INFO - Eval finished!\n",
      "2025-01-09 07:20:05 (UTC) - 0:14:19 - train - INFO - step: 000150 - eval_perplexity: 1.184 - eval_loss: 0.243 - train_loss: 0.048\n",
      "2025-01-09 07:20:05 (UTC) - 0:14:19 - train - INFO - step: 000150 - done (%): 50.0 - loss: 0.048 - lr: 1.2e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 688.9 - avg_words_per_second: 2442.8 - ETA: >2025-01-09 07:28:28\n",
      "2025-01-09 07:20:36 (UTC) - 0:14:49 - train - INFO - step: 000160 - done (%): 53.3 - loss: 0.047 - lr: 1.1e-05 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2673.0 - avg_words_per_second: 2455.8 - ETA: >2025-01-09 07:28:23\n",
      "2025-01-09 07:21:06 (UTC) - 0:15:20 - train - INFO - step: 000170 - done (%): 56.7 - loss: 0.045 - lr: 9.4e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2668.0 - avg_words_per_second: 2466.8 - ETA: >2025-01-09 07:28:18\n",
      "2025-01-09 07:21:37 (UTC) - 0:15:51 - train - INFO - step: 000180 - done (%): 60.0 - loss: 0.050 - lr: 8.3e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2634.6 - avg_words_per_second: 2476.1 - ETA: >2025-01-09 07:28:14\n",
      "2025-01-09 07:21:37 (UTC) - 0:15:51 - checkpointing - INFO - Dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000180/consolidated using tmp name: tmp.consolidated\n",
      "2025-01-09 07:22:06 (UTC) - 0:16:20 - checkpointing - INFO - Done dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000180/consolidated for step: 180\n",
      "2025-01-09 07:22:06 (UTC) - 0:16:20 - checkpointing - INFO - Done deleting checkpoints \n",
      "2025-01-09 07:22:06 (UTC) - 0:16:20 - checkpointing - INFO - Done!\n",
      "2025-01-09 07:22:38 (UTC) - 0:16:52 - train - INFO - step: 000190 - done (%): 63.3 - loss: 0.052 - lr: 7.1e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2601.8 - avg_words_per_second: 2482.4 - ETA: >2025-01-09 07:28:41\n",
      "2025-01-09 07:23:09 (UTC) - 0:17:23 - eval - INFO - Start eval...\n",
      "2025-01-09 07:23:18 (UTC) - 0:17:32 - eval - INFO - Eval finished!\n",
      "2025-01-09 07:23:18 (UTC) - 0:17:32 - train - INFO - step: 000200 - eval_perplexity: 1.196 - eval_loss: 0.259 - train_loss: 0.049\n",
      "2025-01-09 07:23:18 (UTC) - 0:17:32 - train - INFO - step: 000200 - done (%): 66.7 - loss: 0.049 - lr: 6.0e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 688.3 - avg_words_per_second: 2455.6 - ETA: >2025-01-09 07:28:52\n",
      "2025-01-09 07:23:49 (UTC) - 0:18:03 - train - INFO - step: 000210 - done (%): 70.0 - loss: 0.049 - lr: 5.0e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2642.8 - avg_words_per_second: 2463.5 - ETA: >2025-01-09 07:28:48\n",
      "2025-01-09 07:24:20 (UTC) - 0:18:34 - train - INFO - step: 000220 - done (%): 73.3 - loss: 0.039 - lr: 4.0e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2605.0 - avg_words_per_second: 2470.2 - ETA: >2025-01-09 07:28:46\n",
      "2025-01-09 07:24:51 (UTC) - 0:19:05 - train - INFO - step: 000230 - done (%): 76.7 - loss: 0.031 - lr: 3.1e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2652.1 - avg_words_per_second: 2477.2 - ETA: >2025-01-09 07:28:43\n",
      "2025-01-09 07:25:22 (UTC) - 0:19:36 - train - INFO - step: 000240 - done (%): 80.0 - loss: 0.063 - lr: 2.3e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2660.3 - avg_words_per_second: 2483.8 - ETA: >2025-01-09 07:28:40\n",
      "2025-01-09 07:25:22 (UTC) - 0:19:36 - checkpointing - INFO - Dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000240/consolidated using tmp name: tmp.consolidated\n",
      "2025-01-09 07:25:51 (UTC) - 0:20:05 - checkpointing - INFO - Done dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000240/consolidated for step: 240\n",
      "2025-01-09 07:25:52 (UTC) - 0:20:05 - checkpointing - INFO - Deleted ckpt: /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000060\n",
      "2025-01-09 07:25:52 (UTC) - 0:20:05 - checkpointing - INFO - Done deleting checkpoints /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000060\n",
      "2025-01-09 07:25:52 (UTC) - 0:20:05 - checkpointing - INFO - Done!\n",
      "2025-01-09 07:26:23 (UTC) - 0:20:37 - eval - INFO - Start eval...\n",
      "2025-01-09 07:26:32 (UTC) - 0:20:45 - eval - INFO - Eval finished!\n",
      "2025-01-09 07:26:32 (UTC) - 0:20:45 - train - INFO - step: 000250 - eval_perplexity: 1.217 - eval_loss: 0.283 - train_loss: 0.049\n",
      "2025-01-09 07:26:32 (UTC) - 0:20:45 - train - INFO - step: 000250 - done (%): 83.3 - loss: 0.049 - lr: 1.6e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 685.5 - avg_words_per_second: 2463.0 - ETA: >2025-01-09 07:29:18\n",
      "2025-01-09 07:27:03 (UTC) - 0:21:17 - train - INFO - step: 000260 - done (%): 86.7 - loss: 0.047 - lr: 1.1e-06 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2673.5 - avg_words_per_second: 2469.1 - ETA: >2025-01-09 07:29:15\n",
      "2025-01-09 07:27:33 (UTC) - 0:21:47 - train - INFO - step: 000270 - done (%): 90.0 - loss: 0.064 - lr: 6.0e-07 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2686.4 - avg_words_per_second: 2476.3 - ETA: >2025-01-09 07:29:13\n",
      "2025-01-09 07:28:04 (UTC) - 0:22:18 - train - INFO - step: 000280 - done (%): 93.3 - loss: 0.046 - lr: 2.7e-07 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2691.7 - avg_words_per_second: 2482.8 - ETA: >2025-01-09 07:29:10\n",
      "2025-01-09 07:28:35 (UTC) - 0:22:48 - train - INFO - step: 000290 - done (%): 96.7 - loss: 0.057 - lr: 6.8e-08 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 2682.4 - avg_words_per_second: 2489.2 - ETA: >2025-01-09 07:29:07\n",
      "2025-01-09 07:29:05 (UTC) - 0:23:19 - eval - INFO - Start eval...\n",
      "2025-01-09 07:29:14 (UTC) - 0:23:28 - eval - INFO - Eval finished!\n",
      "2025-01-09 07:29:14 (UTC) - 0:23:28 - train - INFO - step: 000300 - eval_perplexity: 1.233 - eval_loss: 0.302 - train_loss: 0.051\n",
      "2025-01-09 07:29:14 (UTC) - 0:23:28 - train - INFO - step: 000300 - done (%): 100.0 - loss: 0.051 - lr: 8.0e-11 - peak_alloc_mem (GB): 42.5 - alloc_mem (GB): 15.8 - words_per_second: 697.9 - avg_words_per_second: 2472.9 - ETA: >2025-01-09 07:29:14\n",
      "2025-01-09 07:29:14 (UTC) - 0:23:28 - checkpointing - INFO - Dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000300/consolidated using tmp name: tmp.consolidated\n",
      "2025-01-09 07:29:43 (UTC) - 0:23:57 - checkpointing - INFO - Done dumping checkpoint in /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000300/consolidated for step: 300\n",
      "2025-01-09 07:29:43 (UTC) - 0:23:57 - checkpointing - INFO - Deleted ckpt: /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000120\n",
      "2025-01-09 07:29:43 (UTC) - 0:23:57 - checkpointing - INFO - Done deleting checkpoints /teamspace/studios/this_studio/output_mixed_data_v2/checkpoints/checkpoint_000120\n",
      "2025-01-09 07:29:43 (UTC) - 0:23:57 - checkpointing - INFO - Done!\n",
      "2025-01-09 07:29:43 (UTC) - 0:23:57 - train - INFO - done!\n",
      "2025-01-09 07:29:43 (UTC) - 0:23:57 - utils - INFO - Closing: eval_logger\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/eval_loss â–â–„â–…â–…â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/perplexity â–â–„â–…â–…â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/train_loss â–ˆâ–„â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/allocated_mem â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/avg_wps â–â–…â–‡â–‡â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/eta_in_seconds â–ˆâ–‡â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lr â–ƒâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/peak_allocated_mem â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/percent_done â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wps â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/eval_loss 0.30179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/perplexity 1.23267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/train_loss 0.05116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/allocated_mem 15.83973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/avg_wps 2472.93726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/eta_in_seconds 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0.05116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/peak_allocated_mem 42.49648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/percent_done 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wps 697.89053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mfine-tuning-mistral\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ibaraahekal/Mistral-7B-v3/runs/4tt2b50b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ibaraahekal/Mistral-7B-v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m/teamspace/studios/this_studio/output_mixed_data_v2/wandb/run-20250109_070551-4tt2b50b/logs\u001b[0m\n",
      "2025-01-09 07:29:44 (UTC) - 0:23:58 - utils - INFO - Closed: eval_logger\n",
      "2025-01-09 07:29:44 (UTC) - 0:23:58 - utils - INFO - Closing: metrics_logger\n",
      "2025-01-09 07:29:44 (UTC) - 0:23:58 - utils - INFO - Closed: metrics_logger\n",
      "2025-01-09 07:29:44 (UTC) - 0:23:58 - train - INFO - Closed everything!\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "!torchrun --nproc-per-node 1 -m train example.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Mistral model from /teamspace/studios/this_studio/mistral_models\n",
      "[INFO] Loading tokenizer from /teamspace/studios/this_studio/mistral_models/tokenizer.model.v3\n",
      "[INFO] Reading test data from /teamspace/studios/this_studio/mistral-finetune/data/test.model-agnostic.json\n",
      "[INFO] Starting inference loop ...\n",
      "Processing Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [08:52<00:00,  2.81sample/s]\n",
      "[INFO] Writing predictions to base_mistral_agnostic.json\n",
      "[INFO] Done! Inference complete.\n"
     ]
    }
   ],
   "source": [
    "!python mistral-finetune/evaluate.py \\\n",
    "    --model_path /teamspace/studios/this_studio/mistral_models \\\n",
    "    --tokenizer_path /teamspace/studios/this_studio/mistral_models/tokenizer.model.v3 \\\n",
    "    --test_file /teamspace/studios/this_studio/mistral-finetune/data/test.model-agnostic.json \\\n",
    "    --output_file base_mistral_agnostic.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pkill -f python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bc8d44e5a84721bb5d37feff5f7a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afb560befb44ef8b8d21debf8152017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0499263c37f5472cbc305bdff084e29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0648acff10d44006a00bc983b8401690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa872850d5a43419afc0128b5babf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6122eb3cab03479dbfb68921ed6b938f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa208d19c48846b18418167a3ecb8000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8d3c1f2fcf4830982d3c8fcb7233a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/mistral_models/7B-v0.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", local_dir=mistral_models_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
